{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf987bb6",
   "metadata": {},
   "source": [
    "%%html\n",
    "\n",
    "<style>\n",
    "    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');\n",
    "    \n",
    "    .ph-header {\n",
    "        background: linear-gradient(135deg, #DA552F 0%, #FF6154 100%);\n",
    "        padding: 60px 40px;\n",
    "        border-radius: 12px;\n",
    "        text-align: center;\n",
    "        margin: 30px 0;\n",
    "        box-shadow: 0 8px 32px rgba(218, 85, 47, 0.2);\n",
    "    }\n",
    "    \n",
    "    .ph-header h1 {\n",
    "        color: white;\n",
    "        font-family: 'Inter', sans-serif;\n",
    "        font-size: 52px;\n",
    "        font-weight: 700;\n",
    "        margin: 0 0 12px 0;\n",
    "        letter-spacing: -0.5px;\n",
    "    }\n",
    "    \n",
    "    .ph-header p {\n",
    "        color: rgba(255, 255, 255, 0.95);\n",
    "        font-family: 'Inter', sans-serif;\n",
    "        font-size: 18px;\n",
    "        font-weight: 400;\n",
    "        margin: 0;\n",
    "        line-height: 1.6;\n",
    "    }\n",
    "    \n",
    "    .info-card {\n",
    "        background: #f8f9fa;\n",
    "        border-left: 3px solid #DA552F;\n",
    "        padding: 16px 20px;\n",
    "        border-radius: 6px;\n",
    "        margin: 20px 0;\n",
    "    }\n",
    "    \n",
    "    .info-card h3 {\n",
    "        color: #DA552F;\n",
    "        margin: 0 0 12px 0;\n",
    "        font-size: 16px;\n",
    "        font-weight: 600;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"ph-header\">\n",
    "    <h1>üöÄ ProductHuntDB</h1>\n",
    "    <p>Product Hunt GraphQL API Data Sink & Kaggle Dataset Manager</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cd43c9",
   "metadata": {},
   "source": [
    "# üìñ Overview\n",
    "\n",
    "This notebook demonstrates how to use **ProductHuntDB** to create, update, and manage a comprehensive Product Hunt dataset on Kaggle.\n",
    "\n",
    "## ‚ú® Key Features\n",
    "\n",
    "- üîç **Harvest** data from Product Hunt GraphQL API (posts, users, topics, collections, comments, votes)\n",
    "- üíæ **Store** in optimized SQLite database with normalized schema\n",
    "- üîÑ **Sync** incrementally with safety margins to avoid data loss\n",
    "- üì§ **Publish** to Kaggle with automatic versioning\n",
    "- ‚úÖ **Validate** data with Pydantic v2 type-safe models\n",
    "\n",
    "<div class=\"info-card\">\n",
    "    <h3>üéØ What You'll Learn</h3>\n",
    "    <ul style=\"margin: 0; padding-left: 20px;\">\n",
    "        <li>Configure ProductHuntDB for Kaggle Notebooks</li>\n",
    "        <li>Initialize database and verify API connections</li>\n",
    "        <li>Perform full and incremental data syncs</li>\n",
    "        <li>Export data to CSV and publish to Kaggle</li>\n",
    "        <li>Query and analyze Product Hunt data</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**üìö Resources:** [GitHub](https://github.com/wyattowalsh/producthuntdb) ‚Ä¢ [Product Hunt API Docs](https://api.producthunt.com/v2/docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033c0a16",
   "metadata": {},
   "source": [
    "<div class=\"info-card\">\n",
    "    <h3>üìÖ Usage Strategy</h3>\n",
    "    <p><strong>First Run (Initial Extraction):</strong></p>\n",
    "    <ul style=\"margin: 0 0 10px 0; padding-left: 20px;\">\n",
    "        <li>Run cell 9 with <code>!producthuntdb sync --full-refresh</code></li>\n",
    "        <li>This downloads all historical Product Hunt data (may take several hours)</li>\n",
    "    </ul>\n",
    "    <p><strong>Scheduled Daily Updates (Kaggle Cron):</strong></p>\n",
    "    <ul style=\"margin: 0; padding-left: 20px;\">\n",
    "        <li>Use default <code>!producthuntdb sync</code> (already configured in cell 9)</li>\n",
    "        <li>Fetches only new data since last run (fast, typically under 5 minutes)</li>\n",
    "        <li>Schedule: Notebook ‚Üí Schedule ‚Üí Daily</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f26a93",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£ Installation & Setup\n",
    "\n",
    "Install ProductHuntDB and configure the environment. This cell automatically detects whether you're running on Kaggle or locally and uses the appropriate installation method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e188662c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing ProductHuntDB...\n",
      "‚ö†Ô∏è  Detected non-standard Python environment.\n",
      "   If using uv, run: uv sync\n",
      "   Otherwise, run: pip install git+https://github.com/wyattowalsh/producthuntdb.git\n",
      "\n",
      "üîß Configuring environment...\n",
      "‚úÖ API token loaded from environment\n",
      "\n",
      "üìÇ Working directory: /Users/ww/dev/projects/producthuntdb/notebooks\n",
      "üíæ Database: /Users/ww/dev/projects/producthuntdb/notebooks/producthunt.db\n",
      "üì§ Export: /Users/ww/dev/projects/producthuntdb/notebooks/export\n"
     ]
    }
   ],
   "source": [
    "# Install ProductHuntDB with comprehensive error handling\n",
    "# Works in Kaggle notebooks and standard Python environments\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üì¶ Installing ProductHuntDB and dependencies...\")\n",
    "\n",
    "# Check if we're in a Kaggle environment or standard Python environment\n",
    "is_kaggle = Path(\"/kaggle/working\").exists()\n",
    "\n",
    "try:\n",
    "    if is_kaggle or \"pip\" in subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"--version\"], \n",
    "        capture_output=True, text=True\n",
    "    ).stdout:\n",
    "        # Standard pip installation (works in Kaggle and most Python envs)\n",
    "        try:\n",
    "            subprocess.check_call([\n",
    "                sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "                \"git+https://github.com/wyattowalsh/producthuntdb.git\"\n",
    "            ], stderr=subprocess.DEVNULL)\n",
    "            print(\"‚úÖ Installed ProductHuntDB from GitHub\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            # Fallback to PyPI (if published)\n",
    "            try:\n",
    "                subprocess.check_call([\n",
    "                    sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"producthuntdb\"\n",
    "                ], stderr=subprocess.DEVNULL)\n",
    "                print(\"‚úÖ Installed ProductHuntDB from PyPI\")\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(\"‚ùå Installation failed. Please install manually.\")\n",
    "                print(\"   Run: pip install git+https://github.com/wyattowalsh/producthuntdb.git\")\n",
    "                raise RuntimeError(\"Failed to install ProductHuntDB\") from e\n",
    "        \n",
    "        # Install additional dependencies for notebook (not in core package)\n",
    "        print(\"üì¶ Installing notebook-specific dependencies...\")\n",
    "        try:\n",
    "            subprocess.check_call([\n",
    "                sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "                \"plotly\", \"kaleido\"  # plotly for interactive viz, kaleido for static export\n",
    "            ], stderr=subprocess.DEVNULL)\n",
    "            print(\"‚úÖ Installed plotly and kaleido\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(\"‚ö†Ô∏è  Optional dependencies (plotly) failed to install\")\n",
    "            print(\"   Visualizations may not work, but core functionality is OK\")\n",
    "    else:\n",
    "        # For local development with uv or similar package managers\n",
    "        print(\"‚ö†Ô∏è  Detected non-standard Python environment.\")\n",
    "        print(\"   If using uv, run: uv sync --group notebook\")\n",
    "        print(\"   Otherwise, run: pip install git+https://github.com/wyattowalsh/producthuntdb.git plotly\")\n",
    "\n",
    "    # Configure paths\n",
    "    print(\"\\nüîß Configuring environment...\")\n",
    "    WORKING_DIR = Path(\"/kaggle/working\") if is_kaggle else Path.cwd()\n",
    "    os.environ[\"DB_PATH\"] = str(WORKING_DIR / \"producthunt.db\")\n",
    "    os.environ[\"EXPORT_DIR\"] = str(WORKING_DIR / \"export\")\n",
    "\n",
    "    # Load secrets from Kaggle Secrets or environment\n",
    "    token_loaded = False\n",
    "    kaggle_configured = False\n",
    "    \n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        user_secrets = UserSecretsClient()\n",
    "        \n",
    "        # Get Product Hunt token (required)\n",
    "        try:\n",
    "            producthunt_token = user_secrets.get_secret(\"PRODUCTHUNT_TOKEN\")\n",
    "            os.environ[\"PRODUCTHUNT_TOKEN\"] = producthunt_token\n",
    "            print(\"‚úÖ PRODUCTHUNT_TOKEN loaded from Kaggle Secrets\")\n",
    "            token_loaded = True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  PRODUCTHUNT_TOKEN not found in Kaggle Secrets: {e}\")\n",
    "        \n",
    "        # Get Kaggle publishing credentials (optional)\n",
    "        try:\n",
    "            kaggle_username = user_secrets.get_secret(\"KAGGLE_USERNAME\")\n",
    "            kaggle_key = user_secrets.get_secret(\"KAGGLE_KEY\")\n",
    "            kaggle_slug = user_secrets.get_secret(\"KAGGLE_DATASET_SLUG\")\n",
    "            \n",
    "            os.environ[\"KAGGLE_USERNAME\"] = kaggle_username\n",
    "            os.environ[\"KAGGLE_KEY\"] = kaggle_key\n",
    "            os.environ[\"KAGGLE_DATASET_SLUG\"] = kaggle_slug\n",
    "            \n",
    "            print(\"‚úÖ Kaggle publishing credentials loaded from Secrets\")\n",
    "            kaggle_configured = True\n",
    "        except Exception:\n",
    "            print(\"‚ÑπÔ∏è  Kaggle publishing credentials not configured (optional)\")\n",
    "            \n",
    "    except ImportError:\n",
    "        # Not in Kaggle environment, try environment variables\n",
    "        print(\"‚ÑπÔ∏è  Not in Kaggle environment, checking environment variables...\")\n",
    "        \n",
    "        producthunt_token = os.getenv(\"PRODUCTHUNT_TOKEN\")\n",
    "        if producthunt_token:\n",
    "            print(\"‚úÖ PRODUCTHUNT_TOKEN loaded from environment\")\n",
    "            token_loaded = True\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  PRODUCTHUNT_TOKEN not found in environment\")\n",
    "\n",
    "    # Summary of configuration\n",
    "    print(f\"\\nüìÇ Working directory: {WORKING_DIR}\")\n",
    "    print(f\"üíæ Database: {os.environ['DB_PATH']}\")\n",
    "    print(f\"üì§ Export: {os.environ['EXPORT_DIR']}\")\n",
    "    \n",
    "    if not token_loaded:\n",
    "        print(\"\\nüö® CRITICAL: No PRODUCTHUNT_TOKEN configured!\")\n",
    "        print(\"   ‚Üí On Kaggle: Add secret in Notebook Settings ‚Üí Add-ons ‚Üí Secrets\")\n",
    "        print(\"   ‚Üí Secret name: PRODUCTHUNT_TOKEN\")\n",
    "        print(\"   ‚Üí Get token at: https://api.producthunt.com/v2/oauth/applications\")\n",
    "        print(\"   ‚ö†Ô∏è  Pipeline will fail without this token!\")\n",
    "    \n",
    "    if not kaggle_configured:\n",
    "        print(\"\\n‚ÑπÔ∏è  Kaggle dataset publishing not configured (optional)\")\n",
    "        print(\"   To enable, add these secrets:\")\n",
    "        print(\"   ‚Ä¢ KAGGLE_USERNAME\")\n",
    "        print(\"   ‚Ä¢ KAGGLE_KEY\")\n",
    "        print(\"   ‚Ä¢ KAGGLE_DATASET_SLUG\")\n",
    "    \n",
    "    # Verify key imports work\n",
    "    print(\"\\nüîç Verifying installation...\")\n",
    "    try:\n",
    "        import producthuntdb\n",
    "        print(\"‚úÖ producthuntdb module imported successfully\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Failed to import producthuntdb: {e}\")\n",
    "        raise\n",
    "    \n",
    "    try:\n",
    "        import plotly\n",
    "        print(\"‚úÖ plotly module imported successfully\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è  plotly not available - visualizations will be limited\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    if token_loaded:\n",
    "        print(\"‚úÖ SETUP COMPLETE - Ready to proceed!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  SETUP INCOMPLETE - Configure PRODUCTHUNT_TOKEN before proceeding\")\n",
    "    print(\"=\" * 60)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Setup failed with error: {str(e)}\")\n",
    "    print(\"   Check your environment configuration and try again.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c04aea2",
   "metadata": {},
   "source": [
    "## üîê Configuration\n",
    "\n",
    "### Required: Product Hunt API Token\n",
    "\n",
    "ProductHuntDB requires a Product Hunt API token to access the GraphQL API.\n",
    "\n",
    "**On Kaggle:**\n",
    "\n",
    "1. Go to **Notebook Settings** ‚Üí **Add-ons** ‚Üí **Secrets**\n",
    "2. Add secret: `PRODUCTHUNT_TOKEN` = Your API token from [api.producthunt.com](https://api.producthunt.com/v2/oauth/applications)\n",
    "\n",
    "**For local development:** Create `.env` file with `PRODUCTHUNT_TOKEN=your_token_here`\n",
    "\n",
    "<div class=\"info-card\">\n",
    "    <h3>‚ö†Ô∏è Security Note</h3>\n",
    "    Never commit API tokens to version control! Always use environment variables or Kaggle Secrets.\n",
    "</div>\n",
    "\n",
    "### Optional: Kaggle Publishing\n",
    "\n",
    "To publish datasets to Kaggle, add these additional secrets:\n",
    "\n",
    "- `KAGGLE_USERNAME` - Your Kaggle username\n",
    "- `KAGGLE_KEY` - Your Kaggle API key (from kaggle.com/settings)\n",
    "- `KAGGLE_DATASET_SLUG` - Dataset slug (format: `username/dataset-name`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782affde",
   "metadata": {},
   "source": [
    "## ‚úÖ Configuration Validation\n",
    "\n",
    "Let's validate your configuration before proceeding with the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9046a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration validation and health check\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "print(\"üîç Validating configuration...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "validation_results = {\n",
    "    'critical': [],\n",
    "    'warnings': [],\n",
    "    'info': []\n",
    "}\n",
    "\n",
    "# 1. Check Python environment\n",
    "print(\"1Ô∏è‚É£  Python Environment\")\n",
    "print(\"-\" * 60)\n",
    "python_version = f\"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\"\n",
    "print(f\"   Python version: {python_version}\")\n",
    "validation_results['info'].append(f\"Python {python_version}\")\n",
    "\n",
    "if sys.version_info >= (3, 11):\n",
    "    print(\"   ‚úÖ Python version compatible\")\n",
    "else:\n",
    "    validation_results['warnings'].append(\"Python <3.11 (recommended 3.11+)\")\n",
    "    print(\"   ‚ö†Ô∏è  Python 3.11+ recommended for best performance\")\n",
    "\n",
    "# 2. Check required environment variables\n",
    "print(\"\\n2Ô∏è‚É£  Environment Variables\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Product Hunt Token\n",
    "ph_token = os.getenv(\"PRODUCTHUNT_TOKEN\")\n",
    "if ph_token:\n",
    "    masked_token = ph_token[:8] + \"...\" + ph_token[-4:] if len(ph_token) > 12 else \"***\"\n",
    "    print(f\"   PRODUCTHUNT_TOKEN: {masked_token}\")\n",
    "    print(\"   ‚úÖ API token configured\")\n",
    "else:\n",
    "    validation_results['critical'].append(\"PRODUCTHUNT_TOKEN not set\")\n",
    "    print(\"   ‚ùå PRODUCTHUNT_TOKEN: Not set\")\n",
    "    print(\"      ‚Üí Add in Kaggle Secrets: Settings ‚Üí Add-ons ‚Üí Secrets\")\n",
    "\n",
    "# Database path\n",
    "db_path = Path(os.getenv(\"DB_PATH\", \"\"))\n",
    "if db_path:\n",
    "    print(f\"   DB_PATH: {db_path}\")\n",
    "    if db_path.parent.exists():\n",
    "        print(f\"   ‚úÖ Database directory exists\")\n",
    "    else:\n",
    "        validation_results['warnings'].append(f\"Database directory doesn't exist: {db_path.parent}\")\n",
    "        print(f\"   ‚ö†Ô∏è  Directory doesn't exist (will be created)\")\n",
    "else:\n",
    "    validation_results['critical'].append(\"DB_PATH not configured\")\n",
    "    print(\"   ‚ùå DB_PATH: Not configured\")\n",
    "\n",
    "# Export directory\n",
    "export_dir = Path(os.getenv(\"EXPORT_DIR\", \"\"))\n",
    "if export_dir:\n",
    "    print(f\"   EXPORT_DIR: {export_dir}\")\n",
    "    if export_dir.exists():\n",
    "        print(\"   ‚úÖ Export directory exists\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Will be created on first export\")\n",
    "else:\n",
    "    validation_results['warnings'].append(\"EXPORT_DIR not configured\")\n",
    "\n",
    "# 3. Check optional Kaggle publishing credentials\n",
    "print(\"\\n3Ô∏è‚É£  Kaggle Publishing (Optional)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "kaggle_username = os.getenv(\"KAGGLE_USERNAME\")\n",
    "kaggle_key = os.getenv(\"KAGGLE_KEY\")\n",
    "kaggle_slug = os.getenv(\"KAGGLE_DATASET_SLUG\")\n",
    "\n",
    "publishing_enabled = all([kaggle_username, kaggle_key, kaggle_slug])\n",
    "\n",
    "if publishing_enabled:\n",
    "    print(f\"   KAGGLE_USERNAME: {kaggle_username}\")\n",
    "    print(f\"   KAGGLE_KEY: {'*' * 20}\")\n",
    "    print(f\"   KAGGLE_DATASET_SLUG: {kaggle_slug}\")\n",
    "    print(\"   ‚úÖ Publishing enabled\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Not configured (dataset publishing disabled)\")\n",
    "    print(\"   ‚Üí To enable: Add KAGGLE_USERNAME, KAGGLE_KEY, KAGGLE_DATASET_SLUG to Secrets\")\n",
    "\n",
    "# 4. Check disk space\n",
    "print(\"\\n4Ô∏è‚É£  System Resources\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    import shutil\n",
    "    working_dir = Path(\"/kaggle/working\") if Path(\"/kaggle/working\").exists() else Path.cwd()\n",
    "    disk_stats = shutil.disk_usage(working_dir)\n",
    "    \n",
    "    free_gb = disk_stats.free / (1024**3)\n",
    "    total_gb = disk_stats.total / (1024**3)\n",
    "    used_percent = (disk_stats.used / disk_stats.total) * 100\n",
    "    \n",
    "    print(f\"   Disk space: {free_gb:.1f} GB free / {total_gb:.1f} GB total ({used_percent:.1f}% used)\")\n",
    "    \n",
    "    if free_gb < 1:\n",
    "        validation_results['critical'].append(f\"Low disk space: {free_gb:.1f} GB\")\n",
    "        print(\"   ‚ùå Low disk space - may cause failures\")\n",
    "    elif free_gb < 5:\n",
    "        validation_results['warnings'].append(f\"Limited disk space: {free_gb:.1f} GB\")\n",
    "        print(\"   ‚ö†Ô∏è  Limited disk space\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Sufficient disk space\")\n",
    "except Exception as e:\n",
    "    validation_results['warnings'].append(f\"Could not check disk space: {e}\")\n",
    "    print(f\"   ‚ö†Ô∏è  Could not check disk space: {e}\")\n",
    "\n",
    "# 5. Check package installation\n",
    "print(\"\\n5Ô∏è‚É£  Package Installation\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    import producthuntdb\n",
    "    version = getattr(producthuntdb, '__version__', 'unknown')\n",
    "    print(f\"   ProductHuntDB version: {version}\")\n",
    "    print(\"   ‚úÖ Package installed correctly\")\n",
    "except ImportError:\n",
    "    validation_results['critical'].append(\"ProductHuntDB not installed\")\n",
    "    print(\"   ‚ùå ProductHuntDB not found\")\n",
    "    print(\"      ‚Üí Re-run installation cell\")\n",
    "\n",
    "# Optional dependencies for enhanced features\n",
    "optional_deps = {\n",
    "    'plotly': 'Interactive visualizations',\n",
    "    'pyarrow': 'Parquet export format'\n",
    "}\n",
    "\n",
    "for package, description in optional_deps.items():\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"   ‚úÖ {package}: Installed ({description})\")\n",
    "    except ImportError:\n",
    "        validation_results['info'].append(f\"Optional: {package} not installed\")\n",
    "        print(f\"   ‚ÑπÔ∏è  {package}: Not installed ({description} disabled)\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä VALIDATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if validation_results['critical']:\n",
    "    print(\"\\nüö® CRITICAL ISSUES (Must fix before proceeding):\")\n",
    "    for issue in validation_results['critical']:\n",
    "        print(f\"   ‚Ä¢ {issue}\")\n",
    "\n",
    "if validation_results['warnings']:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNINGS (Recommended to address):\")\n",
    "    for warning in validation_results['warnings']:\n",
    "        print(f\"   ‚Ä¢ {warning}\")\n",
    "\n",
    "if validation_results['info']:\n",
    "    print(\"\\n‚ÑπÔ∏è  INFORMATIONAL:\")\n",
    "    for info in validation_results['info']:\n",
    "        print(f\"   ‚Ä¢ {info}\")\n",
    "\n",
    "# Overall status\n",
    "if not validation_results['critical']:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ CONFIGURATION VALID - Ready to proceed!\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚ùå CONFIGURATION INVALID - Fix critical issues above\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nüí° Quick fixes:\")\n",
    "    if \"PRODUCTHUNT_TOKEN\" in str(validation_results['critical']):\n",
    "        print(\"   1. Go to Notebook Settings ‚Üí Add-ons ‚Üí Secrets\")\n",
    "        print(\"   2. Add secret: PRODUCTHUNT_TOKEN = <your_token>\")\n",
    "        print(\"   3. Get token from: https://api.producthunt.com/v2/oauth/applications\")\n",
    "        print(\"   4. Re-run this cell to validate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed533cdb",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£ Initialize Database & Verify Connection\n",
    "\n",
    "Let's initialize the database and verify our API authentication works correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ec3038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize database and verify authentication with error handling\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"‚è±Ô∏è  Expected runtime: ~10-30 seconds\\n\")\n",
    "\n",
    "try:\n",
    "    # Initialize database\n",
    "    print(\"üîß Initializing database...\")\n",
    "    result = subprocess.run(\n",
    "        [\"producthuntdb\", \"init\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=False\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Database initialized successfully\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Database initialization encountered issues:\")\n",
    "        print(result.stderr)\n",
    "        if \"already exists\" in result.stderr.lower():\n",
    "            print(\"   (Database may already be initialized - this is usually fine)\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"Database init failed: {result.stderr}\")\n",
    "    \n",
    "    # Verify API authentication\n",
    "    print(\"\\nüîê Verifying API authentication...\")\n",
    "    result = subprocess.run(\n",
    "        [\"producthuntdb\", \"verify\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=False\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ API authentication verified\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(\"‚ùå API authentication failed:\")\n",
    "        print(result.stderr)\n",
    "        print(\"\\nüí° Troubleshooting:\")\n",
    "        print(\"   1. Check your PRODUCTHUNT_TOKEN is valid\")\n",
    "        print(\"   2. Get a new token at: https://api.producthunt.com/v2/oauth/applications\")\n",
    "        print(\"   3. Verify the token is correctly set in Kaggle Secrets or environment\")\n",
    "        raise RuntimeError(f\"API verification failed: {result.stderr}\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå 'producthuntdb' command not found!\")\n",
    "    print(\"   The package may not be installed correctly.\")\n",
    "    print(\"   Try re-running the installation cell above.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Initialization failed: {str(e)}\")\n",
    "    print(\"   Check logs above for specific error details.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c57ebd",
   "metadata": {},
   "source": [
    "# 3Ô∏è‚É£ Sync Data from Product Hunt\n",
    "\n",
    "Sync data from the Product Hunt API. Start with a limited sync to test, then perform a full sync when ready.\n",
    "\n",
    "**Sync Options:**\n",
    "\n",
    "- **Full Refresh** - Downloads all historical data (can take several hours)\n",
    "- **Incremental Update** - Only syncs new/updated data since last run (fast)\n",
    "- **Limited Sync** - Fetches a specific number of pages (good for testing)\n",
    "\n",
    "<div class=\"info-card\">\n",
    "    <h3>üí° Recommended Approach</h3>\n",
    "    Start with <code>--max-pages 10</code> to test the setup, then run a full sync: <code>producthuntdb sync --full-refresh</code>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8786a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync data from Product Hunt with comprehensive error handling\n",
    "# Configure sync strategy based on your needs\n",
    "import subprocess\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"‚è±Ô∏è  Expected runtime:\")\n",
    "print(\"   ‚Ä¢ Full refresh: 2-4 hours (first run)\")\n",
    "print(\"   ‚Ä¢ Incremental: 3-5 minutes (daily updates)\")\n",
    "print(\"   ‚Ä¢ Limited test: 1-2 minutes (--max-pages 10)\\n\")\n",
    "\n",
    "# Track sync timing\n",
    "start_time = datetime.now()\n",
    "print(f\"üöÄ Starting sync at {start_time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "try:\n",
    "    # üéØ FOR FIRST RUN: Uncomment this line to get all historical data\n",
    "    # sync_command = [\"producthuntdb\", \"sync\", \"--full-refresh\"]\n",
    "    \n",
    "    # üîÑ FOR SCHEDULED DAILY UPDATES: Use this (default, fast incremental updates)\n",
    "    sync_command = [\"producthuntdb\", \"sync\"]\n",
    "    \n",
    "    # üß™ FOR TESTING: Limit to a few pages (uncomment to use)\n",
    "    # sync_command = [\"producthuntdb\", \"sync\", \"--max-pages\", \"10\"]\n",
    "    \n",
    "    # üìä POSTS ONLY: Skip topics and collections (faster, uncomment to use)\n",
    "    # sync_command = [\"producthuntdb\", \"sync\", \"--posts-only\"]\n",
    "    \n",
    "    print(f\"üì° Running command: {' '.join(sync_command)}\\n\")\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        sync_command,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=False,\n",
    "        timeout=14400  # 4-hour timeout (Kaggle limit is 12 hours)\n",
    "    )\n",
    "    \n",
    "    # Calculate elapsed time\n",
    "    end_time = datetime.now()\n",
    "    elapsed = end_time - start_time\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(result.stdout)\n",
    "        print(f\"\\n‚úÖ Sync completed successfully in {elapsed.total_seconds():.1f} seconds\")\n",
    "        print(f\"   ({elapsed.total_seconds() / 60:.1f} minutes)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Sync encountered errors:\")\n",
    "        print(result.stderr)\n",
    "        \n",
    "        # Provide context-specific troubleshooting\n",
    "        if \"rate limit\" in result.stderr.lower():\n",
    "            print(\"\\nüí° Rate Limit Hit - Troubleshooting:\")\n",
    "            print(\"   ‚Ä¢ The API has rate limits that reset periodically\")\n",
    "            print(\"   ‚Ä¢ Built-in retry logic will handle this automatically\")\n",
    "            print(\"   ‚Ä¢ For faster testing, use --max-pages option\")\n",
    "            print(\"   ‚Ä¢ Consider running sync during off-peak hours\")\n",
    "        elif \"timeout\" in result.stderr.lower():\n",
    "            print(\"\\nüí° Timeout - Troubleshooting:\")\n",
    "            print(\"   ‚Ä¢ Full refresh can take several hours\")\n",
    "            print(\"   ‚Ä¢ Use incremental sync for daily updates\")\n",
    "            print(\"   ‚Ä¢ Data collected before timeout is safely stored\")\n",
    "            print(\"   ‚Ä¢ Re-run to continue from where it left off\")\n",
    "        elif \"authentication\" in result.stderr.lower() or \"token\" in result.stderr.lower():\n",
    "            print(\"\\nüí° Authentication Error - Troubleshooting:\")\n",
    "            print(\"   ‚Ä¢ Verify PRODUCTHUNT_TOKEN is set correctly\")\n",
    "            print(\"   ‚Ä¢ Token may have expired - get new one from api.producthunt.com\")\n",
    "            print(\"   ‚Ä¢ Check Kaggle Secrets configuration\")\n",
    "        else:\n",
    "            print(\"\\nüí° General Troubleshooting:\")\n",
    "            print(\"   ‚Ä¢ Check database file is not corrupted\")\n",
    "            print(\"   ‚Ä¢ Verify sufficient disk space available\")\n",
    "            print(\"   ‚Ä¢ Review full error message above\")\n",
    "            print(\"   ‚Ä¢ Try running 'producthuntdb status' to check database state\")\n",
    "        \n",
    "        # Don't raise if partial success (some data may have been synced)\n",
    "        if \"error\" in result.stderr.lower() and result.stdout:\n",
    "            print(\"\\n‚ö†Ô∏è  Partial sync completed - some data was saved before error\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"Sync failed: {result.stderr}\")\n",
    "    \n",
    "    # Save sync timing for performance monitoring\n",
    "    try:\n",
    "        with open(\"/kaggle/working/sync_history.txt\" if Path(\"/kaggle/working\").exists() else \"sync_history.txt\", \"a\") as f:\n",
    "            f.write(f\"{start_time.isoformat()},{elapsed.total_seconds()},{result.returncode}\\n\")\n",
    "    except Exception:\n",
    "        pass  # Non-critical if we can't save timing data\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    end_time = datetime.now()\n",
    "    elapsed = end_time - start_time\n",
    "    print(f\"\\n‚è±Ô∏è  Sync timed out after {elapsed.total_seconds() / 3600:.1f} hours\")\n",
    "    print(\"   Data collected up to this point has been saved to the database.\")\n",
    "    print(\"   You can re-run sync to continue where it left off.\")\n",
    "    print(\"\\nüí° To avoid timeouts:\")\n",
    "    print(\"   ‚Ä¢ Use incremental sync instead of --full-refresh\")\n",
    "    print(\"   ‚Ä¢ Run during off-peak hours\")\n",
    "    print(\"   ‚Ä¢ Consider splitting into smaller batches with --max-pages\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå 'producthuntdb' command not found!\")\n",
    "    print(\"   Re-run the installation cell to fix this.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    end_time = datetime.now()\n",
    "    elapsed = end_time - start_time\n",
    "    print(f\"\\n‚ùå Sync failed after {elapsed.total_seconds():.1f} seconds\")\n",
    "    print(f\"   Error: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b0206",
   "metadata": {},
   "source": [
    "# 4Ô∏è‚É£ Database Statistics & Status\n",
    "\n",
    "Let's examine what we've collected and view key statistics about the database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2e920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View database statistics\n",
    "!producthuntdb status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b9132c",
   "metadata": {},
   "source": [
    "# üìã Data Quality Validation\n",
    "\n",
    "Let's validate the data quality with comprehensive checks for completeness, freshness, and integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63138206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality validation\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"‚è±Ô∏è  Expected runtime: ~10-15 seconds\\n\")\n",
    "print(\"üîç Running data quality checks...\\n\")\n",
    "\n",
    "try:\n",
    "    db_path = Path(os.environ.get('DB_PATH', '/kaggle/working/producthunt.db'))\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # 1. Data Completeness Check\n",
    "    print(\"=\" * 60)\n",
    "    print(\"1Ô∏è‚É£  DATA COMPLETENESS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    completeness = pd.read_sql_query(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            'Posts' as entity,\n",
    "            COUNT(*) as total_records,\n",
    "            SUM(CASE WHEN name IS NULL THEN 1 ELSE 0 END) as missing_name,\n",
    "            SUM(CASE WHEN tagline IS NULL THEN 1 ELSE 0 END) as missing_tagline,\n",
    "            SUM(CASE WHEN featured_at IS NULL THEN 1 ELSE 0 END) as missing_date,\n",
    "            ROUND(100.0 * SUM(CASE WHEN name IS NOT NULL THEN 1 ELSE 0 END) / COUNT(*), 2) as name_completeness\n",
    "        FROM post_row\n",
    "        UNION ALL\n",
    "        SELECT \n",
    "            'Users',\n",
    "            COUNT(*),\n",
    "            SUM(CASE WHEN name IS NULL THEN 1 ELSE 0 END),\n",
    "            SUM(CASE WHEN username IS NULL THEN 1 ELSE 0 END),\n",
    "            SUM(CASE WHEN created_at IS NULL THEN 1 ELSE 0 END),\n",
    "            ROUND(100.0 * SUM(CASE WHEN name IS NOT NULL THEN 1 ELSE 0 END) / COUNT(*), 2)\n",
    "        FROM user_row\n",
    "        UNION ALL\n",
    "        SELECT \n",
    "            'Topics',\n",
    "            COUNT(*),\n",
    "            SUM(CASE WHEN name IS NULL THEN 1 ELSE 0 END),\n",
    "            SUM(CASE WHEN slug IS NULL THEN 1 ELSE 0 END),\n",
    "            0,\n",
    "            ROUND(100.0 * SUM(CASE WHEN name IS NOT NULL THEN 1 ELSE 0 END) / COUNT(*), 2)\n",
    "        FROM topic_row\n",
    "    \"\"\",\n",
    "        conn,\n",
    "    )\n",
    "    display(completeness)\n",
    "    \n",
    "    # Flag critical issues\n",
    "    critical_issues = []\n",
    "    for _, row in completeness.iterrows():\n",
    "        if row['name_completeness'] < 95:\n",
    "            critical_issues.append(f\"‚ö†Ô∏è  {row['entity']}: Only {row['name_completeness']}% name completeness\")\n",
    "    \n",
    "    if critical_issues:\n",
    "        print(\"\\nüö® CRITICAL ISSUES DETECTED:\")\n",
    "        for issue in critical_issues:\n",
    "            print(f\"   {issue}\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ All entities have >95% completeness\")\n",
    "    \n",
    "    # 2. Data Freshness Check\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2Ô∏è‚É£  DATA FRESHNESS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    freshness = pd.read_sql_query(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            MAX(DATE(featured_at)) as latest_post,\n",
    "            MIN(DATE(featured_at)) as earliest_post,\n",
    "            COUNT(*) as total_posts,\n",
    "            JULIANDAY('now') - JULIANDAY(MAX(featured_at)) as days_since_latest\n",
    "        FROM post_row\n",
    "        WHERE featured_at IS NOT NULL\n",
    "    \"\"\",\n",
    "        conn,\n",
    "    )\n",
    "    \n",
    "    if not freshness.empty:\n",
    "        display(freshness)\n",
    "        \n",
    "        days_stale = freshness['days_since_latest'].iloc[0]\n",
    "        if days_stale > 2:\n",
    "            print(f\"\\n‚ö†Ô∏è  Data is {days_stale:.1f} days old - consider running sync\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ Data is fresh ({days_stale:.1f} days old)\")\n",
    "    \n",
    "    # 3. Duplicate Detection\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3Ô∏è‚É£  DUPLICATE DETECTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    duplicates = pd.read_sql_query(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            'Posts' as entity,\n",
    "            COUNT(*) - COUNT(DISTINCT id) as duplicate_ids,\n",
    "            COUNT(*) - COUNT(DISTINCT name) as duplicate_names\n",
    "        FROM post_row\n",
    "        UNION ALL\n",
    "        SELECT \n",
    "            'Users',\n",
    "            COUNT(*) - COUNT(DISTINCT id),\n",
    "            COUNT(*) - COUNT(DISTINCT username)\n",
    "        FROM user_row\n",
    "        UNION ALL\n",
    "        SELECT \n",
    "            'Topics',\n",
    "            COUNT(*) - COUNT(DISTINCT id),\n",
    "            COUNT(*) - COUNT(DISTINCT slug)\n",
    "        FROM topic_row\n",
    "    \"\"\",\n",
    "        conn,\n",
    "    )\n",
    "    display(duplicates)\n",
    "    \n",
    "    total_duplicates = duplicates['duplicate_ids'].sum()\n",
    "    if total_duplicates > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  Found {total_duplicates} duplicate IDs - may need data cleaning\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No duplicate IDs detected\")\n",
    "    \n",
    "    # 4. Relationship Integrity\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4Ô∏è‚É£  RELATIONSHIP INTEGRITY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    integrity = pd.read_sql_query(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            'Orphaned Comments' as check_type,\n",
    "            COUNT(*) as count\n",
    "        FROM comment_row c\n",
    "        WHERE NOT EXISTS (SELECT 1 FROM post_row p WHERE p.id = c.post_id)\n",
    "        UNION ALL\n",
    "        SELECT \n",
    "            'Orphaned Votes',\n",
    "            COUNT(*)\n",
    "        FROM vote_row v\n",
    "        WHERE NOT EXISTS (SELECT 1 FROM post_row p WHERE p.id = v.post_id)\n",
    "        UNION ALL\n",
    "        SELECT \n",
    "            'Orphaned Maker Links',\n",
    "            COUNT(*)\n",
    "        FROM maker_post_link mpl\n",
    "        WHERE NOT EXISTS (SELECT 1 FROM post_row p WHERE p.id = mpl.post_id)\n",
    "           OR NOT EXISTS (SELECT 1 FROM user_row u WHERE u.id = mpl.maker_id)\n",
    "    \"\"\",\n",
    "        conn,\n",
    "    )\n",
    "    display(integrity)\n",
    "    \n",
    "    integrity_issues = integrity[integrity['count'] > 0]\n",
    "    if len(integrity_issues) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  Found {len(integrity_issues)} integrity issues\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ All relationships are valid\")\n",
    "    \n",
    "    # 5. Data Distribution Analysis\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5Ô∏è‚É£  DATA DISTRIBUTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    distribution = pd.read_sql_query(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            strftime('%Y-%m', featured_at) as month,\n",
    "            COUNT(*) as posts,\n",
    "            AVG(votes_count) as avg_votes,\n",
    "            MAX(votes_count) as max_votes\n",
    "        FROM post_row\n",
    "        WHERE featured_at >= date('now', '-12 months')\n",
    "        GROUP BY month\n",
    "        ORDER BY month DESC\n",
    "        LIMIT 12\n",
    "    \"\"\",\n",
    "        conn,\n",
    "    )\n",
    "    \n",
    "    if not distribution.empty:\n",
    "        display(distribution)\n",
    "        \n",
    "        # Check for anomalies\n",
    "        if distribution['posts'].std() / distribution['posts'].mean() > 0.5:\n",
    "            print(\"\\n‚ö†Ô∏è  High variance in monthly post counts - data may be incomplete\")\n",
    "        else:\n",
    "            print(\"\\n‚úÖ Consistent data distribution\")\n",
    "    \n",
    "    # 6. Quality Score Summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä OVERALL QUALITY SCORE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    quality_metrics = {\n",
    "        'Completeness': 100 - (completeness['missing_name'].sum() / completeness['total_records'].sum() * 100),\n",
    "        'Freshness': max(0, 100 - (days_stale * 10)),\n",
    "        'Uniqueness': 100 - (total_duplicates / completeness['total_records'].sum() * 100) if completeness['total_records'].sum() > 0 else 100,\n",
    "        'Integrity': 100 - (integrity['count'].sum() / completeness['total_records'].sum() * 100) if completeness['total_records'].sum() > 0 else 100\n",
    "    }\n",
    "    \n",
    "    quality_df = pd.DataFrame(list(quality_metrics.items()), columns=['Metric', 'Score'])\n",
    "    quality_df['Score'] = quality_df['Score'].round(2)\n",
    "    display(quality_df)\n",
    "    \n",
    "    overall_score = sum(quality_metrics.values()) / len(quality_metrics)\n",
    "    print(f\"\\nüéØ Overall Data Quality Score: {overall_score:.1f}/100\")\n",
    "    \n",
    "    if overall_score >= 90:\n",
    "        print(\"   ‚úÖ EXCELLENT - Data is production-ready\")\n",
    "    elif overall_score >= 75:\n",
    "        print(\"   ‚ö†Ô∏è  GOOD - Minor issues detected, consider review\")\n",
    "    elif overall_score >= 60:\n",
    "        print(\"   ‚ö†Ô∏è  FAIR - Notable issues, recommend data cleaning\")\n",
    "    else:\n",
    "        print(\"   üö® POOR - Critical issues, data quality needs attention\")\n",
    "    \n",
    "    # Create quality score visualization\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        x=quality_df['Metric'],\n",
    "        y=quality_df['Score'],\n",
    "        marker_color=['#2ecc71' if s >= 90 else '#f39c12' if s >= 75 else '#e74c3c' for s in quality_df['Score']],\n",
    "        text=quality_df['Score'].round(1),\n",
    "        textposition='auto',\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Data Quality Metrics Dashboard',\n",
    "        xaxis_title='Quality Dimension',\n",
    "        yaxis_title='Score (0-100)',\n",
    "        yaxis_range=[0, 105],\n",
    "        showlegend=False,\n",
    "        hovermode='x'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    conn.close()\n",
    "    print(\"\\n‚úÖ Data quality validation complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Validation failed: {str(e)}\")\n",
    "    raise\n",
    "finally:\n",
    "    try:\n",
    "        conn.close()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ba878",
   "metadata": {},
   "source": [
    "# 5Ô∏è‚É£ Query & Analyze Data\n",
    "\n",
    "Let's explore the data we've collected with some SQL queries using pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b5c7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query and analyze data with memory-efficient patterns\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"‚è±Ô∏è  Expected runtime: ~5-10 seconds\\n\")\n",
    "\n",
    "try:\n",
    "    # Connect to the database\n",
    "    db_path = Path(os.environ.get('DB_PATH', '/kaggle/working/producthunt.db'))\n",
    "    \n",
    "    if not db_path.exists():\n",
    "        print(\"‚ùå Database not found! Run the sync cell first.\")\n",
    "        raise FileNotFoundError(f\"Database not found at {db_path}\")\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # Use chunksize for large queries to avoid memory issues\n",
    "    CHUNK_SIZE = 10000\n",
    "    \n",
    "    print(\"üèÜ Top 10 Products by Votes\\n\")\n",
    "    top_posts = pd.read_sql_query(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            name,\n",
    "            tagline,\n",
    "            votes_count,\n",
    "            comments_count,\n",
    "            DATE(featured_at) as featured_date,\n",
    "            url\n",
    "        FROM post_row\n",
    "        WHERE votes_count IS NOT NULL\n",
    "        ORDER BY votes_count DESC\n",
    "        LIMIT 10\n",
    "    \"\"\",\n",
    "        conn,\n",
    "    )\n",
    "    \n",
    "    if top_posts.empty:\n",
    "        print(\"‚ö†Ô∏è  No posts found. Run sync to populate database.\")\n",
    "    else:\n",
    "        # Format for better display\n",
    "        top_posts['featured_date'] = pd.to_datetime(top_posts['featured_date'])\n",
    "        display(top_posts)\n",
    "    \n",
    "    print(\"\\nüë§ Top 10 Most Active Makers\\n\")\n",
    "    active_makers = pd.read_sql_query(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            u.name,\n",
    "            u.username,\n",
    "            COUNT(DISTINCT mpl.post_id) as products_made,\n",
    "            u.url,\n",
    "            u.headline\n",
    "        FROM user_row u\n",
    "        JOIN maker_post_link mpl ON u.id = mpl.maker_id\n",
    "        GROUP BY u.id\n",
    "        ORDER BY products_made DESC\n",
    "        LIMIT 10\n",
    "    \"\"\",\n",
    "        conn,\n",
    "    )\n",
    "    \n",
    "    if active_makers.empty:\n",
    "        print(\"‚ö†Ô∏è  No maker data found. Ensure sync completed successfully.\")\n",
    "    else:\n",
    "        display(active_makers)\n",
    "    \n",
    "    print(\"\\nüè∑Ô∏è  Top 10 Popular Topics\\n\")\n",
    "    popular_topics = pd.read_sql_query(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            t.name,\n",
    "            t.slug,\n",
    "            COUNT(DISTINCT ptl.post_id) as product_count,\n",
    "            t.description\n",
    "        FROM topic_row t\n",
    "        JOIN post_topic_link ptl ON t.id = ptl.topic_id\n",
    "        GROUP BY t.id\n",
    "        ORDER BY product_count DESC\n",
    "        LIMIT 10\n",
    "    \"\"\",\n",
    "        conn,\n",
    "    )\n",
    "    \n",
    "    if popular_topics.empty:\n",
    "        print(\"‚ö†Ô∏è  No topic data found. Topics may not have been synced.\")\n",
    "    else:\n",
    "        display(popular_topics)\n",
    "    \n",
    "    # Additional analytics: Recent high-engagement products\n",
    "    print(\"\\nüî• Recently Launched High-Engagement Products (Last 30 Days)\\n\")\n",
    "    recent_hot = pd.read_sql_query(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            name,\n",
    "            tagline,\n",
    "            votes_count,\n",
    "            comments_count,\n",
    "            DATE(featured_at) as launch_date,\n",
    "            (votes_count + comments_count * 2) as engagement_score\n",
    "        FROM post_row\n",
    "        WHERE featured_at >= date('now', '-30 days')\n",
    "            AND votes_count > 0\n",
    "        ORDER BY engagement_score DESC\n",
    "        LIMIT 10\n",
    "    \"\"\",\n",
    "        conn,\n",
    "    )\n",
    "    \n",
    "    if not recent_hot.empty:\n",
    "        recent_hot['launch_date'] = pd.to_datetime(recent_hot['launch_date'])\n",
    "        display(recent_hot)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No recent products found. Database may need more data.\")\n",
    "    \n",
    "    # Database statistics\n",
    "    print(\"\\nüìä Database Statistics\\n\")\n",
    "    stats = pd.read_sql_query(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            'Posts' as table_name, COUNT(*) as row_count FROM post_row\n",
    "        UNION ALL\n",
    "        SELECT 'Users', COUNT(*) FROM user_row\n",
    "        UNION ALL\n",
    "        SELECT 'Topics', COUNT(*) FROM topic_row\n",
    "        UNION ALL\n",
    "        SELECT 'Comments', COUNT(*) FROM comment_row\n",
    "        UNION ALL\n",
    "        SELECT 'Collections', COUNT(*) FROM collection_row\n",
    "        UNION ALL\n",
    "        SELECT 'Votes', COUNT(*) FROM vote_row\n",
    "    \"\"\",\n",
    "        conn,\n",
    "    )\n",
    "    display(stats)\n",
    "    \n",
    "    conn.close()\n",
    "    print(\"\\n‚úÖ Query analysis complete!\")\n",
    "    \n",
    "except sqlite3.Error as e:\n",
    "    print(f\"‚ùå Database error: {str(e)}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"   ‚Ä¢ Database may be locked by another process\")\n",
    "    print(\"   ‚Ä¢ Try closing other connections to the database\")\n",
    "    print(\"   ‚Ä¢ Run 'producthuntdb status' to check database health\")\n",
    "    raise\n",
    "except pd.errors.DatabaseError as e:\n",
    "    print(f\"‚ùå Query error: {str(e)}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"   ‚Ä¢ Database schema may be outdated\")\n",
    "    print(\"   ‚Ä¢ Try running 'producthuntdb upgrade head' to apply migrations\")\n",
    "    raise\n",
    "except MemoryError:\n",
    "    print(\"‚ùå Out of memory!\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"   ‚Ä¢ Query returned too much data\")\n",
    "    print(\"   ‚Ä¢ Use LIMIT clauses to reduce result size\")\n",
    "    print(\"   ‚Ä¢ Consider using chunked reading for large datasets\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Query failed: {str(e)}\")\n",
    "    raise\n",
    "finally:\n",
    "    # Ensure connection is closed\n",
    "    try:\n",
    "        conn.close()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33119921",
   "metadata": {},
   "source": [
    "## üî¨ Advanced Query Examples\n",
    "\n",
    "Explore Product Hunt data with sophisticated analytical queries for deeper insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cada083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced analytical queries for Product Hunt insights\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"‚è±Ô∏è  Expected runtime: ~10-15 seconds\\n\")\n",
    "print(\"üî¨ Running advanced analytics...\\n\")\n",
    "\n",
    "try:\n",
    "    db_path = Path(os.environ.get('DB_PATH', '/kaggle/working/producthunt.db'))\n",
    "    if not db_path.exists():\n",
    "        print(\"‚ùå Database not found! Run sync first.\")\n",
    "        raise FileNotFoundError(f\"Database not found at {db_path}\")\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # 1. Maker Network Analysis - Find prolific makers and their collaborators\n",
    "    print(\"=\" * 60)\n",
    "    print(\"1Ô∏è‚É£  MAKER NETWORK ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    maker_network = pd.read_sql_query(\n",
    "        \"\"\"\n",
    "        WITH maker_stats AS (\n",
    "            SELECT \n",
    "                u.id,\n",
    "                u.name,\n",
    "                u.username,\n",
    "                COUNT(DISTINCT mpl.post_id) as products_made,\n",
    "                AVG(p.votes_count) as avg_votes_per_product,\n",
    "                MAX(p.votes_count) as best_product_votes,\n",
    "                COUNT(DISTINCT CASE WHEN p.featured_at >= date('now', '-90 days') THEN mpl.post_id END) as recent_products\n",
    "            FROM user_row u\n",
    "            JOIN maker_post_link mpl ON u.id = mpl.maker_id\n",
    "            JOIN post_row p ON mpl.post_id = p.id\n",
    "            GROUP BY u.id\n",
    "            HAVING products_made >= 3\n",
    "        )\n",
    "        SELECT *\n",
    "        FROM maker_stats\n",
    "        ORDER BY products_made DESC, avg_votes_per_product DESC\n",
    "        LIMIT 20\n",
    "    \"\"\",\n",
    "        conn\n",
    "    )\n",
    "    \n",
    "    if not maker_network.empty:\n",
    "        print(f\"\\nüìä Top 20 Prolific Makers (3+ products):\\n\")\n",
    "        maker_network['avg_votes_per_product'] = maker_network['avg_votes_per_product'].round(1)\n",
    "        display(maker_network)\n",
    "        \n",
    "        # Visualize maker productivity\n",
    "        fig1 = px.scatter(\n",
    "            maker_network,\n",
    "            x='products_made',\n",
    "            y='avg_votes_per_product',\n",
    "            size='best_product_votes',\n",
    "            hover_data=['name', 'username', 'recent_products'],\n",
    "            title='Maker Productivity: Quantity vs Quality',\n",
    "            labels={\n",
    "                'products_made': 'Number of Products',\n",
    "                'avg_votes_per_product': 'Average Votes per Product'\n",
    "            },\n",
    "            color='recent_products',\n",
    "            color_continuous_scale='Viridis'\n",
    "        )\n",
    "        fig1.show()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Insufficient data for maker network analysis\")\n",
    "    \n",
    "    # 2. Topic Evolution - Trending topics over time\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2Ô∏è‚É£  TOPIC EVOLUTION & TRENDS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    topic_trends = pd.read_sql_query(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            t.name as topic,\n",
    "            strftime('%Y-%m', p.featured_at) as month,\n",
    "            COUNT(DISTINCT p.id) as product_count,\n",
    "            AVG(p.votes_count) as avg_votes\n",
    "        FROM topic_row t\n",
    "        JOIN post_topic_link ptl ON t.id = ptl.topic_id\n",
    "        JOIN post_row p ON ptl.post_id = p.id\n",
    "        WHERE p.featured_at >= date('now', '-12 months')\n",
    "            AND t.name IN (\n",
    "                SELECT t2.name \n",
    "                FROM topic_row t2\n",
    "                JOIN post_topic_link ptl2 ON t2.id = ptl2.topic_id\n",
    "                GROUP BY t2.id\n",
    "                ORDER BY COUNT(DISTINCT ptl2.post_id) DESC\n",
    "                LIMIT 8\n",
    "            )\n",
    "        GROUP BY t.name, month\n",
    "        ORDER BY month DESC, product_count DESC\n",
    "    \"\"\",\n",
    "        conn\n",
    "    )\n",
    "    \n",
    "    if not topic_trends.empty:\n",
    "        # Pivot for heatmap\n",
    "        topic_pivot = topic_trends.pivot(index='topic', columns='month', values='product_count')\n",
    "        topic_pivot = topic_pivot.fillna(0)\n",
    "        \n",
    "        fig2 = go.Figure(data=go.Heatmap(\n",
    "            z=topic_pivot.values,\n",
    "            x=topic_pivot.columns,\n",
    "            y=topic_pivot.index,\n",
    "            colorscale='YlOrRd',\n",
    "            hoverongaps=False\n",
    "        ))\n",
    "        \n",
    "        fig2.update_layout(\n",
    "            title='Topic Popularity Heatmap (Last 12 Months)',\n",
    "            xaxis_title='Month',\n",
    "            yaxis_title='Topic',\n",
    "            height=500\n",
    "        )\n",
    "        fig2.show()\n",
    "        \n",
    "        print(f\"\\nüìà Topic trends data:\")\n",
    "        display(topic_trends.head(20))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Insufficient data for topic trends\")\n",
    "    \n",
    "    # 3. Launch Timing Analysis - Best days and times to launch\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3Ô∏è‚É£  OPTIMAL LAUNCH TIMING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    launch_timing = pd.read_sql_query(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            CASE CAST(strftime('%w', featured_at) AS INTEGER)\n",
    "                WHEN 0 THEN 'Sunday'\n",
    "                WHEN 1 THEN 'Monday'\n",
    "                WHEN 2 THEN 'Tuesday'\n",
    "                WHEN 3 THEN 'Wednesday'\n",
    "                WHEN 4 THEN 'Thursday'\n",
    "                WHEN 5 THEN 'Friday'\n",
    "                WHEN 6 THEN 'Saturday'\n",
    "            END as day_of_week,\n",
    "            CAST(strftime('%w', featured_at) AS INTEGER) as day_num,\n",
    "            CAST(strftime('%H', featured_at) AS INTEGER) as hour,\n",
    "            COUNT(*) as launch_count,\n",
    "            AVG(votes_count) as avg_votes,\n",
    "            AVG(comments_count) as avg_comments,\n",
    "            AVG(votes_count + comments_count * 2) as avg_engagement\n",
    "        FROM post_row\n",
    "        WHERE featured_at IS NOT NULL\n",
    "            AND featured_at >= date('now', '-180 days')\n",
    "        GROUP BY day_of_week, day_num, hour\n",
    "        ORDER BY day_num, hour\n",
    "    \"\"\",\n",
    "        conn\n",
    "    )\n",
    "    \n",
    "    if not launch_timing.empty:\n",
    "        # Create heatmap of best launch times\n",
    "        timing_pivot = launch_timing.pivot_table(\n",
    "            index='hour',\n",
    "            columns='day_of_week',\n",
    "            values='avg_engagement',\n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        # Reorder columns to standard week order\n",
    "        day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        timing_pivot = timing_pivot[[col for col in day_order if col in timing_pivot.columns]]\n",
    "        \n",
    "        fig3 = go.Figure(data=go.Heatmap(\n",
    "            z=timing_pivot.values,\n",
    "            x=timing_pivot.columns,\n",
    "            y=timing_pivot.index,\n",
    "            colorscale='RdYlGn',\n",
    "            hoverongaps=False\n",
    "        ))\n",
    "        \n",
    "        fig3.update_layout(\n",
    "            title='Best Launch Times (Engagement Score = Votes + Comments√ó2)',\n",
    "            xaxis_title='Day of Week',\n",
    "            yaxis_title='Hour (UTC)',\n",
    "            height=600\n",
    "        )\n",
    "        fig3.show()\n",
    "        \n",
    "        # Find top 5 best time slots\n",
    "        best_times = launch_timing.nlargest(5, 'avg_engagement')[\n",
    "            ['day_of_week', 'hour', 'avg_engagement', 'launch_count']\n",
    "        ]\n",
    "        print(\"\\nüéØ Top 5 Launch Time Slots:\")\n",
    "        display(best_times)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Insufficient data for timing analysis\")\n",
    "    \n",
    "    # 4. Product Success Patterns - What makes products successful?\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4Ô∏è‚É£  SUCCESS PATTERN ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    success_patterns = pd.read_sql_query(\n",
    "        \"\"\"\n",
    "        WITH product_metrics AS (\n",
    "            SELECT \n",
    "                p.id,\n",
    "                p.name,\n",
    "                p.votes_count,\n",
    "                p.comments_count,\n",
    "                LENGTH(p.tagline) as tagline_length,\n",
    "                LENGTH(p.description) as description_length,\n",
    "                COUNT(DISTINCT mpl.maker_id) as maker_count,\n",
    "                COUNT(DISTINCT ptl.topic_id) as topic_count,\n",
    "                CASE \n",
    "                    WHEN p.votes_count >= (SELECT PERCENTILE_CONT(0.9) WITHIN GROUP (ORDER BY votes_count) FROM post_row)\n",
    "                    THEN 'Top 10%'\n",
    "                    WHEN p.votes_count >= (SELECT PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY votes_count) FROM post_row)\n",
    "                    THEN 'Top 25%'\n",
    "                    WHEN p.votes_count >= (SELECT PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY votes_count) FROM post_row)\n",
    "                    THEN 'Top 50%'\n",
    "                    ELSE 'Bottom 50%'\n",
    "                END as success_tier\n",
    "            FROM post_row p\n",
    "            LEFT JOIN maker_post_link mpl ON p.id = mpl.post_id\n",
    "            LEFT JOIN post_topic_link ptl ON p.id = ptl.post_id\n",
    "            WHERE p.votes_count > 0\n",
    "            GROUP BY p.id\n",
    "        )\n",
    "        SELECT \n",
    "            success_tier,\n",
    "            COUNT(*) as product_count,\n",
    "            AVG(votes_count) as avg_votes,\n",
    "            AVG(comments_count) as avg_comments,\n",
    "            AVG(tagline_length) as avg_tagline_length,\n",
    "            AVG(maker_count) as avg_makers,\n",
    "            AVG(topic_count) as avg_topics\n",
    "        FROM product_metrics\n",
    "        GROUP BY success_tier\n",
    "        ORDER BY \n",
    "            CASE success_tier\n",
    "                WHEN 'Top 10%' THEN 1\n",
    "                WHEN 'Top 25%' THEN 2\n",
    "                WHEN 'Top 50%' THEN 3\n",
    "                ELSE 4\n",
    "            END\n",
    "    \"\"\",\n",
    "        conn\n",
    "    )\n",
    "    \n",
    "    if not success_patterns.empty:\n",
    "        print(\"\\nüéØ Success Patterns by Performance Tier:\\n\")\n",
    "        display(success_patterns.round(2))\n",
    "        \n",
    "        # Visualize patterns\n",
    "        fig4 = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Average Makers', 'Average Topics', 'Tagline Length', 'Comments Engagement'),\n",
    "            specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "                   [{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
    "        )\n",
    "        \n",
    "        fig4.add_trace(go.Bar(x=success_patterns['success_tier'], y=success_patterns['avg_makers'], \n",
    "                              name='Makers', marker_color='lightblue'), row=1, col=1)\n",
    "        fig4.add_trace(go.Bar(x=success_patterns['success_tier'], y=success_patterns['avg_topics'], \n",
    "                              name='Topics', marker_color='lightgreen'), row=1, col=2)\n",
    "        fig4.add_trace(go.Bar(x=success_patterns['success_tier'], y=success_patterns['avg_tagline_length'], \n",
    "                              name='Tagline Length', marker_color='lightyellow'), row=2, col=1)\n",
    "        fig4.add_trace(go.Bar(x=success_patterns['success_tier'], y=success_patterns['avg_comments'], \n",
    "                              name='Comments', marker_color='lightcoral'), row=2, col=2)\n",
    "        \n",
    "        fig4.update_layout(height=700, title_text=\"Success Pattern Analysis\", showlegend=False)\n",
    "        fig4.show()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Insufficient data for success pattern analysis\")\n",
    "    \n",
    "    # 5. Community Engagement - Most discussed products\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5Ô∏è‚É£  COMMUNITY ENGAGEMENT LEADERS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    engagement = pd.read_sql_query(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            p.name,\n",
    "            p.tagline,\n",
    "            p.votes_count,\n",
    "            p.comments_count,\n",
    "            COUNT(DISTINCT c.user_id) as unique_commenters,\n",
    "            ROUND(CAST(p.comments_count AS FLOAT) / NULLIF(p.votes_count, 0), 3) as comment_to_vote_ratio,\n",
    "            DATE(p.featured_at) as launch_date\n",
    "        FROM post_row p\n",
    "        LEFT JOIN comment_row c ON p.id = c.post_id\n",
    "        WHERE p.comments_count > 10\n",
    "            AND p.votes_count > 50\n",
    "        GROUP BY p.id\n",
    "        ORDER BY p.comments_count DESC, unique_commenters DESC\n",
    "        LIMIT 15\n",
    "    \"\"\",\n",
    "        conn\n",
    "    )\n",
    "    \n",
    "    if not engagement.empty:\n",
    "        print(\"\\nüí¨ Most Discussed Products:\\n\")\n",
    "        display(engagement)\n",
    "        \n",
    "        fig5 = px.scatter(\n",
    "            engagement,\n",
    "            x='votes_count',\n",
    "            y='comments_count',\n",
    "            size='unique_commenters',\n",
    "            hover_data=['name', 'tagline', 'launch_date'],\n",
    "            title='Engagement Patterns: Votes vs Comments',\n",
    "            labels={'votes_count': 'Votes', 'comments_count': 'Comments'},\n",
    "            color='comment_to_vote_ratio',\n",
    "            color_continuous_scale='Turbo'\n",
    "        )\n",
    "        fig5.show()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Insufficient engagement data\")\n",
    "    \n",
    "    conn.close()\n",
    "    print(\"\\n‚úÖ Advanced analytics complete! üéâ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Analysis failed: {str(e)}\")\n",
    "    raise\n",
    "finally:\n",
    "    try:\n",
    "        conn.close()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e3db26",
   "metadata": {},
   "source": [
    "## üìà Visualize Trends\n",
    "\n",
    "Let's create some visualizations to understand Product Hunt trends better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e387388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive visualizations with Plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"‚è±Ô∏è  Expected runtime: ~5-10 seconds\\n\")\n",
    "\n",
    "try:\n",
    "    # Connect to database\n",
    "    db_path = Path(os.environ.get('DB_PATH', '/kaggle/working/producthunt.db'))\n",
    "    \n",
    "    if not db_path.exists():\n",
    "        print(\"‚ùå Database not found! Run the sync cell first.\")\n",
    "        raise FileNotFoundError(f\"Database not found at {db_path}\")\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # Posts by day of week with error handling\n",
    "    print(\"üìÖ Analyzing Product Launches by Day of Week...\")\n",
    "    posts_by_day = pd.read_sql_query(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            CASE CAST(strftime('%w', featured_at) AS INTEGER)\n",
    "                WHEN 0 THEN 'Sunday'\n",
    "                WHEN 1 THEN 'Monday'\n",
    "                WHEN 2 THEN 'Tuesday'\n",
    "                WHEN 3 THEN 'Wednesday'\n",
    "                WHEN 4 THEN 'Thursday'\n",
    "                WHEN 5 THEN 'Friday'\n",
    "                WHEN 6 THEN 'Saturday'\n",
    "            END as day_of_week,\n",
    "            CAST(strftime('%w', featured_at) AS INTEGER) as day_num,\n",
    "            COUNT(*) as count,\n",
    "            AVG(votes_count) as avg_votes,\n",
    "            AVG(comments_count) as avg_comments\n",
    "        FROM post_row\n",
    "        WHERE featured_at IS NOT NULL\n",
    "        GROUP BY day_of_week, day_num\n",
    "        ORDER BY day_num\n",
    "    \"\"\",\n",
    "        conn,\n",
    "    )\n",
    "    \n",
    "    if posts_by_day.empty:\n",
    "        print(\"‚ö†Ô∏è  No data found for visualization. Sync may not have completed.\")\n",
    "    else:\n",
    "        # Create interactive bar chart with Plotly\n",
    "        fig1 = px.bar(\n",
    "            posts_by_day,\n",
    "            x='day_of_week',\n",
    "            y='count',\n",
    "            title='Product Launches by Day of Week',\n",
    "            labels={'count': 'Number of Products', 'day_of_week': 'Day of Week'},\n",
    "            color='count',\n",
    "            color_continuous_scale='Viridis',\n",
    "            hover_data={'count': True, 'avg_votes': ':.1f', 'avg_comments': ':.1f'}\n",
    "        )\n",
    "        fig1.update_layout(\n",
    "            xaxis_title='Day of Week',\n",
    "            yaxis_title='Number of Products',\n",
    "            hovermode='x unified',\n",
    "            coloraxis_showscale=False\n",
    "        )\n",
    "        fig1.show()\n",
    "    \n",
    "    # Votes vs Comments correlation\n",
    "    print(\"\\nüìä Analyzing Engagement Metrics...\")\n",
    "    posts_metrics = pd.read_sql_query(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            name,\n",
    "            tagline,\n",
    "            votes_count,\n",
    "            comments_count,\n",
    "            DATE(featured_at) as launch_date\n",
    "        FROM post_row\n",
    "        WHERE votes_count > 0 AND comments_count > 0\n",
    "        ORDER BY votes_count DESC\n",
    "        LIMIT 1000\n",
    "    \"\"\",\n",
    "        conn,\n",
    "    )\n",
    "    \n",
    "    if posts_metrics.empty:\n",
    "        print(\"‚ö†Ô∏è  Insufficient data for correlation analysis.\")\n",
    "    else:\n",
    "        # Interactive scatter plot with hover info\n",
    "        fig2 = px.scatter(\n",
    "            posts_metrics,\n",
    "            x='votes_count',\n",
    "            y='comments_count',\n",
    "            title='Votes vs Comments Correlation (Top 1000 Products)',\n",
    "            labels={'votes_count': 'Votes Count', 'comments_count': 'Comments Count'},\n",
    "            hover_data=['name', 'tagline', 'launch_date'],\n",
    "            color='votes_count',\n",
    "            size='comments_count',\n",
    "            color_continuous_scale='Reds',\n",
    "            opacity=0.6,\n",
    "            log_x=True,\n",
    "            log_y=True\n",
    "        )\n",
    "        fig2.update_layout(\n",
    "            hovermode='closest',\n",
    "            xaxis_title='Votes Count (log scale)',\n",
    "            yaxis_title='Comments Count (log scale)'\n",
    "        )\n",
    "        fig2.show()\n",
    "    \n",
    "    # Monthly launch trends\n",
    "    print(\"\\nüìà Analyzing Launch Trends Over Time...\")\n",
    "    monthly_trends = pd.read_sql_query(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            strftime('%Y-%m', featured_at) as month,\n",
    "            COUNT(*) as launches,\n",
    "            AVG(votes_count) as avg_votes,\n",
    "            SUM(votes_count) as total_votes\n",
    "        FROM post_row\n",
    "        WHERE featured_at IS NOT NULL\n",
    "        GROUP BY month\n",
    "        ORDER BY month DESC\n",
    "        LIMIT 24\n",
    "    \"\"\",\n",
    "        conn,\n",
    "    )\n",
    "    \n",
    "    if not monthly_trends.empty:\n",
    "        monthly_trends = monthly_trends.sort_values('month')\n",
    "        \n",
    "        # Create dual-axis chart\n",
    "        fig3 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "        \n",
    "        fig3.add_trace(\n",
    "            go.Bar(name='Product Launches', x=monthly_trends['month'], y=monthly_trends['launches'],\n",
    "                   marker_color='lightblue'),\n",
    "            secondary_y=False,\n",
    "        )\n",
    "        \n",
    "        fig3.add_trace(\n",
    "            go.Scatter(name='Average Votes', x=monthly_trends['month'], y=monthly_trends['avg_votes'],\n",
    "                      mode='lines+markers', line=dict(color='red', width=3)),\n",
    "            secondary_y=True,\n",
    "        )\n",
    "        \n",
    "        fig3.update_layout(\n",
    "            title='Product Launch Trends (Last 24 Months)',\n",
    "            hovermode='x unified',\n",
    "            xaxis_title='Month',\n",
    "            legend=dict(x=0.01, y=0.99)\n",
    "        )\n",
    "        fig3.update_yaxes(title_text=\"Number of Launches\", secondary_y=False)\n",
    "        fig3.update_yaxes(title_text=\"Average Votes\", secondary_y=True)\n",
    "        \n",
    "        fig3.show()\n",
    "    \n",
    "    conn.close()\n",
    "    print(\"\\n‚úÖ Visualizations complete! Hover over charts to explore data interactively.\")\n",
    "    \n",
    "except sqlite3.Error as e:\n",
    "    print(f\"‚ùå Database error: {str(e)}\")\n",
    "    print(\"   The database may be corrupted. Try re-running init and sync.\")\n",
    "    raise\n",
    "except pd.errors.DatabaseError as e:\n",
    "    print(f\"‚ùå Query error: {str(e)}\")\n",
    "    print(\"   Check that the database schema is correct.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Visualization failed: {str(e)}\")\n",
    "    print(\"   Check that data exists in the database and Plotly is installed.\")\n",
    "    raise\n",
    "finally:\n",
    "    # Ensure connection is closed\n",
    "    try:\n",
    "        conn.close()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463d87f8",
   "metadata": {},
   "source": [
    "# 6Ô∏è‚É£ Export to CSV\n",
    "\n",
    "Export the database tables to CSV files for easy analysis and sharing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decc6554",
   "metadata": {},
   "source": [
    "# üìà Performance Monitoring Dashboard\n",
    "\n",
    "Track pipeline performance over time to identify trends and optimize scheduled runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb1f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance monitoring and sync history visualization\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import sqlite3\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"‚è±Ô∏è  Expected runtime: ~5-10 seconds\\n\")\n",
    "print(\"üìä Analyzing pipeline performance...\\n\")\n",
    "\n",
    "try:\n",
    "    # Load sync history if available\n",
    "    working_dir = Path(\"/kaggle/working\") if Path(\"/kaggle/working\").exists() else Path.cwd()\n",
    "    history_file = working_dir / \"sync_history.txt\"\n",
    "    \n",
    "    db_path = Path(os.environ.get('DB_PATH', '/kaggle/working/producthunt.db'))\n",
    "    \n",
    "    if history_file.exists():\n",
    "        print(\"=\" * 60)\n",
    "        print(\"üïê SYNC HISTORY\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Parse sync history\n",
    "        history_data = []\n",
    "        with open(history_file, 'r') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    timestamp, duration, exit_code = line.strip().split(',')\n",
    "                    history_data.append({\n",
    "                        'timestamp': pd.to_datetime(timestamp),\n",
    "                        'duration_seconds': float(duration),\n",
    "                        'duration_minutes': float(duration) / 60,\n",
    "                        'success': int(exit_code) == 0\n",
    "                    })\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        if history_data:\n",
    "            df_history = pd.DataFrame(history_data)\n",
    "            df_history['date'] = df_history['timestamp'].dt.date\n",
    "            \n",
    "            print(f\"\\nüìÖ Total sync runs: {len(df_history)}\")\n",
    "            print(f\"‚úÖ Successful: {df_history['success'].sum()}\")\n",
    "            print(f\"‚ùå Failed: {(~df_history['success']).sum()}\")\n",
    "            print(f\"‚è±Ô∏è  Average duration: {df_history['duration_minutes'].mean():.1f} minutes\")\n",
    "            print(f\"‚ö° Fastest run: {df_history['duration_minutes'].min():.1f} minutes\")\n",
    "            print(f\"üêå Slowest run: {df_history['duration_minutes'].max():.1f} minutes\")\n",
    "            \n",
    "            # Create performance visualization\n",
    "            fig = make_subplots(\n",
    "                rows=2, cols=1,\n",
    "                subplot_titles=('Sync Duration Over Time', 'Daily Success Rate'),\n",
    "                vertical_spacing=0.15\n",
    "            )\n",
    "            \n",
    "            # Duration trend\n",
    "            colors = ['green' if s else 'red' for s in df_history['success']]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_history['timestamp'],\n",
    "                    y=df_history['duration_minutes'],\n",
    "                    mode='lines+markers',\n",
    "                    name='Duration',\n",
    "                    marker=dict(color=colors, size=8),\n",
    "                    line=dict(color='blue', width=2)\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "            \n",
    "            # Success rate by day\n",
    "            daily_stats = df_history.groupby('date').agg({\n",
    "                'success': ['sum', 'count']\n",
    "            }).reset_index()\n",
    "            daily_stats.columns = ['date', 'successful', 'total']\n",
    "            daily_stats['success_rate'] = (daily_stats['successful'] / daily_stats['total'] * 100)\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=daily_stats['date'],\n",
    "                    y=daily_stats['success_rate'],\n",
    "                    name='Success Rate',\n",
    "                    marker_color='lightgreen'\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "            \n",
    "            fig.update_xaxes(title_text=\"Time\", row=1, col=1)\n",
    "            fig.update_yaxes(title_text=\"Duration (minutes)\", row=1, col=1)\n",
    "            fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n",
    "            fig.update_yaxes(title_text=\"Success Rate (%)\", row=2, col=1)\n",
    "            \n",
    "            fig.update_layout(\n",
    "                height=700,\n",
    "                title_text=\"Pipeline Performance Metrics\",\n",
    "                showlegend=True,\n",
    "                hovermode='x unified'\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "            \n",
    "            # Recent runs detail\n",
    "            print(\"\\nüìã Recent Sync Runs:\")\n",
    "            recent = df_history.tail(10)[['timestamp', 'duration_minutes', 'success']].copy()\n",
    "            recent['status'] = recent['success'].map({True: '‚úÖ Success', False: '‚ùå Failed'})\n",
    "            recent['duration_minutes'] = recent['duration_minutes'].round(2)\n",
    "            recent = recent[['timestamp', 'duration_minutes', 'status']]\n",
    "            display(recent)\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No valid sync history found\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No sync history file found yet\")\n",
    "        print(\"   History tracking starts after first sync in this session\")\n",
    "    \n",
    "    # Database growth metrics\n",
    "    if db_path.exists():\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üìà DATA GROWTH METRICS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Monthly growth\n",
    "        growth = pd.read_sql_query(\n",
    "            \"\"\"\n",
    "            SELECT \n",
    "                strftime('%Y-%m', featured_at) as month,\n",
    "                COUNT(*) as new_posts,\n",
    "                SUM(COUNT(*)) OVER (ORDER BY strftime('%Y-%m', featured_at)) as cumulative_posts\n",
    "            FROM post_row\n",
    "            WHERE featured_at IS NOT NULL\n",
    "            GROUP BY month\n",
    "            ORDER BY month DESC\n",
    "            LIMIT 12\n",
    "            \"\"\",\n",
    "            conn\n",
    "        )\n",
    "        \n",
    "        if not growth.empty:\n",
    "            growth = growth.sort_values('month')\n",
    "            \n",
    "            # Create growth chart\n",
    "            fig2 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "            \n",
    "            fig2.add_trace(\n",
    "                go.Bar(\n",
    "                    name='New Posts',\n",
    "                    x=growth['month'],\n",
    "                    y=growth['new_posts'],\n",
    "                    marker_color='lightblue'\n",
    "                ),\n",
    "                secondary_y=False\n",
    "            )\n",
    "            \n",
    "            fig2.add_trace(\n",
    "                go.Scatter(\n",
    "                    name='Cumulative Total',\n",
    "                    x=growth['month'],\n",
    "                    y=growth['cumulative_posts'],\n",
    "                    mode='lines+markers',\n",
    "                    line=dict(color='red', width=3)\n",
    "                ),\n",
    "                secondary_y=True\n",
    "            )\n",
    "            \n",
    "            fig2.update_xaxes(title_text=\"Month\")\n",
    "            fig2.update_yaxes(title_text=\"New Posts\", secondary_y=False)\n",
    "            fig2.update_yaxes(title_text=\"Cumulative Total\", secondary_y=True)\n",
    "            \n",
    "            fig2.update_layout(\n",
    "                title_text=\"Database Growth Trend\",\n",
    "                hovermode='x unified',\n",
    "                height=400\n",
    "            )\n",
    "            \n",
    "            fig2.show()\n",
    "            \n",
    "            print(\"\\nüìä Growth Statistics:\")\n",
    "            display(growth)\n",
    "        \n",
    "        # Database file size\n",
    "        db_size_mb = db_path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"\\nüíæ Database file size: {db_size_mb:.2f} MB\")\n",
    "        \n",
    "        # Storage efficiency\n",
    "        total_records = pd.read_sql_query(\n",
    "            \"SELECT SUM(cnt) as total FROM (SELECT COUNT(*) as cnt FROM post_row UNION ALL SELECT COUNT(*) FROM user_row UNION ALL SELECT COUNT(*) FROM topic_row UNION ALL SELECT COUNT(*) FROM comment_row UNION ALL SELECT COUNT(*) FROM vote_row)\",\n",
    "            conn\n",
    "        )['total'].iloc[0]\n",
    "        \n",
    "        bytes_per_record = (db_size_mb * 1024 * 1024) / total_records if total_records > 0 else 0\n",
    "        print(f\"üì¶ Storage efficiency: {bytes_per_record:.0f} bytes/record\")\n",
    "        print(f\"üìù Total records: {total_records:,}\")\n",
    "        \n",
    "        conn.close()\n",
    "    \n",
    "    print(\"\\n‚úÖ Performance monitoring complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Performance monitoring failed: {str(e)}\")\n",
    "    print(\"   This is non-critical - pipeline can continue without monitoring\")\n",
    "finally:\n",
    "    try:\n",
    "        conn.close()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af71ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export database to multiple formats with performance comparison\n",
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚è±Ô∏è  Expected runtime: ~1-3 minutes\\n\")\n",
    "\n",
    "export_dir = Path(os.environ.get('EXPORT_DIR', '/kaggle/working/export'))\n",
    "db_path = Path(os.environ.get('DB_PATH', '/kaggle/working/producthunt.db'))\n",
    "\n",
    "try:\n",
    "    # Export to CSV (default format)\n",
    "    print(\"üì§ Exporting to CSV format...\")\n",
    "    result = subprocess.run(\n",
    "        [\"producthuntdb\", \"export\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=False\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ CSV export completed\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  CSV export encountered issues:\")\n",
    "        print(result.stderr)\n",
    "        if \"database is locked\" in result.stderr.lower():\n",
    "            print(\"\\nüí° Database is locked - close other connections and retry\")\n",
    "        raise RuntimeError(f\"Export failed: {result.stderr}\")\n",
    "    \n",
    "    # Additional exports: Parquet and JSON for different use cases\n",
    "    print(\"\\nüì¶ Creating additional export formats...\")\n",
    "    \n",
    "    if not db_path.exists():\n",
    "        print(\"‚ö†Ô∏è  Database not found, skipping additional exports\")\n",
    "    else:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        # Export key tables to Parquet (efficient for data science)\n",
    "        parquet_dir = export_dir / \"parquet\"\n",
    "        parquet_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(\"   ‚Üí Exporting to Parquet (optimized for data analysis)...\")\n",
    "        tables_to_export = ['post_row', 'user_row', 'topic_row', 'comment_row', 'vote_row']\n",
    "        \n",
    "        for table in tables_to_export:\n",
    "            try:\n",
    "                df = pd.read_sql_query(f\"SELECT * FROM {table}\", conn)\n",
    "                if not df.empty:\n",
    "                    parquet_file = parquet_dir / f\"{table}.parquet\"\n",
    "                    df.to_parquet(parquet_file, index=False, compression='snappy')\n",
    "                    print(f\"      ‚úì {table}.parquet\")\n",
    "            except Exception as e:\n",
    "                print(f\"      ‚úó {table}: {str(e)}\")\n",
    "        \n",
    "        # Export summary to JSON (lightweight API-friendly format)\n",
    "        json_dir = export_dir / \"json\"\n",
    "        json_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(\"   ‚Üí Exporting summaries to JSON (API-friendly)...\")\n",
    "        \n",
    "        # Top products summary\n",
    "        top_products = pd.read_sql_query(\n",
    "            \"\"\"\n",
    "            SELECT \n",
    "                id, name, tagline, votes_count, comments_count,\n",
    "                DATE(featured_at) as featured_date, url\n",
    "            FROM post_row\n",
    "            ORDER BY votes_count DESC\n",
    "            LIMIT 100\n",
    "            \"\"\",\n",
    "            conn\n",
    "        )\n",
    "        if not top_products.empty:\n",
    "            top_products.to_json(json_dir / \"top_products.json\", orient='records', indent=2)\n",
    "            print(\"      ‚úì top_products.json\")\n",
    "        \n",
    "        # Popular topics summary\n",
    "        topics_summary = pd.read_sql_query(\n",
    "            \"\"\"\n",
    "            SELECT \n",
    "                t.name, t.slug, t.description,\n",
    "                COUNT(DISTINCT ptl.post_id) as product_count\n",
    "            FROM topic_row t\n",
    "            LEFT JOIN post_topic_link ptl ON t.id = ptl.topic_id\n",
    "            GROUP BY t.id\n",
    "            ORDER BY product_count DESC\n",
    "            LIMIT 50\n",
    "            \"\"\",\n",
    "            conn\n",
    "        )\n",
    "        if not topics_summary.empty:\n",
    "            topics_summary.to_json(json_dir / \"popular_topics.json\", orient='records', indent=2)\n",
    "            print(\"      ‚úì popular_topics.json\")\n",
    "        \n",
    "        conn.close()\n",
    "    \n",
    "    # List exported files with size comparison\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìÅ EXPORTED FILES SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if export_dir.exists():\n",
    "        file_stats = []\n",
    "        \n",
    "        # CSV files\n",
    "        for csv_file in sorted(export_dir.glob(\"*.csv\")):\n",
    "            size_kb = csv_file.stat().st_size / 1024\n",
    "            file_stats.append({\n",
    "                'Format': 'CSV',\n",
    "                'File': csv_file.name,\n",
    "                'Size (KB)': round(size_kb, 1),\n",
    "                'Size (MB)': round(size_kb / 1024, 2)\n",
    "            })\n",
    "        \n",
    "        # Parquet files\n",
    "        if (export_dir / \"parquet\").exists():\n",
    "            for parquet_file in sorted((export_dir / \"parquet\").glob(\"*.parquet\")):\n",
    "                size_kb = parquet_file.stat().st_size / 1024\n",
    "                file_stats.append({\n",
    "                    'Format': 'Parquet',\n",
    "                    'File': parquet_file.name,\n",
    "                    'Size (KB)': round(size_kb, 1),\n",
    "                    'Size (MB)': round(size_kb / 1024, 2)\n",
    "                })\n",
    "        \n",
    "        # JSON files\n",
    "        if (export_dir / \"json\").exists():\n",
    "            for json_file in sorted((export_dir / \"json\").glob(\"*.json\")):\n",
    "                size_kb = json_file.stat().st_size / 1024\n",
    "                file_stats.append({\n",
    "                    'Format': 'JSON',\n",
    "                    'File': json_file.name,\n",
    "                    'Size (KB)': round(size_kb, 1),\n",
    "                    'Size (MB)': round(size_kb / 1024, 2)\n",
    "                })\n",
    "        \n",
    "        if file_stats:\n",
    "            stats_df = pd.DataFrame(file_stats)\n",
    "            print(\"\\n\")\n",
    "            display(stats_df)\n",
    "            \n",
    "            # Summary by format\n",
    "            print(\"\\nüìä Size Comparison by Format:\")\n",
    "            format_summary = stats_df.groupby('Format')['Size (MB)'].agg(['count', 'sum']).round(2)\n",
    "            format_summary.columns = ['File Count', 'Total Size (MB)']\n",
    "            display(format_summary)\n",
    "            \n",
    "            # Format recommendations\n",
    "            print(\"\\nüí° Format Recommendations:\")\n",
    "            print(\"   ‚Ä¢ CSV: Universal compatibility, human-readable, larger file size\")\n",
    "            print(\"   ‚Ä¢ Parquet: 50-80% smaller, fast loading, best for pandas/data science\")\n",
    "            print(\"   ‚Ä¢ JSON: API-friendly, smaller summaries, good for web applications\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No files found in export directory\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Export directory not found: {export_dir}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Export complete!\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå 'producthuntdb' command not found!\")\n",
    "    print(\"   Re-run the installation cell to fix this.\")\n",
    "    raise\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"‚ùå Export command failed: {str(e)}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Export failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6483dab",
   "metadata": {},
   "source": [
    "# 7Ô∏è‚É£ Publish to Kaggle\n",
    "\n",
    "Publish your dataset to Kaggle! This will create a new dataset or update an existing one.\n",
    "\n",
    "**Prerequisites:**\n",
    "\n",
    "1. `KAGGLE_USERNAME` and `KAGGLE_KEY` - Your Kaggle API credentials\n",
    "2. `KAGGLE_DATASET_SLUG` - Dataset identifier (e.g., `yourusername/product-hunt-database`)\n",
    "\n",
    "**Setup:**\n",
    "\n",
    "1. Go to **Notebook Settings** ‚Üí **Add-ons** ‚Üí **Secrets**\n",
    "2. Add the three secrets listed above\n",
    "\n",
    "<div class=\"info-card\">\n",
    "    <h3>üìù Note</h3>\n",
    "    Publishing from a Kaggle notebook to Kaggle may have limitations. For production use, consider running the publish command from a local environment or CI/CD pipeline.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58f865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publish to Kaggle (requires credentials to be configured)\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    # Check if credentials are already set from installation cell\n",
    "    kaggle_username = os.getenv(\"KAGGLE_USERNAME\")\n",
    "    kaggle_key = os.getenv(\"KAGGLE_KEY\")\n",
    "    kaggle_slug = os.getenv(\"KAGGLE_DATASET_SLUG\")\n",
    "    \n",
    "    if not all([kaggle_username, kaggle_key, kaggle_slug]):\n",
    "        print(\"‚ö†Ô∏è  Kaggle credentials not configured.\")\n",
    "        print(\"   Publishing to Kaggle requires:\")\n",
    "        print(\"   ‚Ä¢ KAGGLE_USERNAME\")\n",
    "        print(\"   ‚Ä¢ KAGGLE_KEY (from kaggle.com/settings)\")\n",
    "        print(\"   ‚Ä¢ KAGGLE_DATASET_SLUG (format: username/dataset-name)\")\n",
    "        print(\"\\n   Add these as Kaggle Secrets and re-run the installation cell.\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Publishing to Kaggle dataset: {kaggle_slug}\\n\")\n",
    "        \n",
    "        result = subprocess.run(\n",
    "            [\"producthuntdb\", \"publish\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=False\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(result.stdout)\n",
    "            print(f\"\\n‚úÖ Dataset published successfully!\")\n",
    "            print(f\"   View at: https://www.kaggle.com/datasets/{kaggle_slug}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Publishing encountered issues:\")\n",
    "            print(result.stderr)\n",
    "            print(\"\\nüí° Troubleshooting:\")\n",
    "            print(\"   ‚Ä¢ Verify Kaggle credentials are correct\")\n",
    "            print(\"   ‚Ä¢ Ensure dataset exists or CLI can create it\")\n",
    "            print(\"   ‚Ä¢ Check you have write permissions\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Publishing failed: {str(e)}\")\n",
    "    print(\"   This is optional - core pipeline functionality is not affected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8c69ff",
   "metadata": {},
   "source": [
    "# 8Ô∏è‚É£ Schedule Automatic Daily Updates\n",
    "\n",
    "**This notebook is production-ready for Kaggle's scheduling feature!** Set it up once and your dataset will stay current automatically.\n",
    "\n",
    "## üöÄ Quick Setup\n",
    "\n",
    "1. **Complete First Run**: Run all cells once with `--full-refresh` to get historical data\n",
    "2. **Enable Scheduling**:\n",
    "   - Click **Notebook** ‚Üí **Schedule Run**\n",
    "   - Select **Daily** (or your preferred frequency)\n",
    "   - Kaggle will run the notebook automatically\n",
    "3. **That's it!** The default `!producthuntdb sync` in cell 9 is already optimized for incremental updates\n",
    "\n",
    "## ‚öôÔ∏è How It Works\n",
    "\n",
    "The notebook is configured to automatically detect whether it needs a full refresh or incremental update:\n",
    "\n",
    "- **First run**: Use `--full-refresh` to populate the database\n",
    "- **Subsequent runs**: Standard `sync` command only fetches new data since last run\n",
    "- **Safety margins**: Built-in 5-minute lookback prevents data loss\n",
    "- **Rate limiting**: Automatic retry logic handles API limits gracefully\n",
    "\n",
    "<div class=\"info-card\">\n",
    "    <h3>üí° Best Practices</h3>\n",
    "    <ul style=\"margin: 0; padding-left: 20px;\">\n",
    "        <li><strong>Initial Setup</strong>: Run with <code>--full-refresh</code> once (cell 9)</li>\n",
    "        <li><strong>Daily Updates</strong>: Use default <code>sync</code> command (no flags)</li>\n",
    "        <li><strong>Monitor Logs</strong>: Check execution history for any errors</li>\n",
    "        <li><strong>Dataset Publishing</strong>: Runs automatically if Kaggle credentials are configured</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "## üìä Expected Performance\n",
    "\n",
    "| Operation    | Duration    | Data                     |\n",
    "| ------------ | ----------- | ------------------------ |\n",
    "| Full Refresh | 2-4 hours   | All historical data      |\n",
    "| Daily Update | 3-5 minutes | New posts since last run |\n",
    "| Export       | 1-2 minutes | All tables to CSV        |\n",
    "| Publish      | 1-2 minutes | Update Kaggle dataset    |\n",
    "\n",
    "Total scheduled run time: **~10 minutes per day**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71521ef8",
   "metadata": {},
   "source": [
    "# üé¨ Complete Workflow Summary\n",
    "\n",
    "Here's the complete ProductHuntDB workflow using the CLI:\n",
    "\n",
    "```bash\n",
    "# 1. Initialize database\n",
    "producthuntdb init\n",
    "\n",
    "# 2. Verify API authentication\n",
    "producthuntdb verify\n",
    "\n",
    "# 3. Sync data (choose one)\n",
    "producthuntdb sync --max-pages 10        # Limited sync (testing)\n",
    "producthuntdb sync --full-refresh        # Full historical harvest\n",
    "producthuntdb sync                       # Incremental update\n",
    "\n",
    "# 4. Check database status\n",
    "producthuntdb status\n",
    "\n",
    "# 5. Export to CSV\n",
    "producthuntdb export\n",
    "\n",
    "# 6. Publish to Kaggle (requires credentials)\n",
    "producthuntdb publish\n",
    "\n",
    "# Advanced: Database migrations\n",
    "producthuntdb migration-history          # View migration history\n",
    "producthuntdb migrate \"description\"      # Create new migration\n",
    "producthuntdb upgrade head               # Apply migrations\n",
    "producthuntdb downgrade -1               # Rollback one revision\n",
    "```\n",
    "\n",
    "## üìö CLI Help\n",
    "\n",
    "For detailed help on any command:\n",
    "\n",
    "```bash\n",
    "producthuntdb --help\n",
    "producthuntdb sync --help\n",
    "producthuntdb export --help\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd5fcf3",
   "metadata": {},
   "source": [
    "# üéì Additional Resources & Troubleshooting\n",
    "\n",
    "## üìñ Documentation\n",
    "\n",
    "- **Full Documentation**: [GitHub Repository](https://github.com/wyattowalsh/producthuntdb)\n",
    "- **API Reference**: [Product Hunt GraphQL API](https://api.producthunt.com/v2/docs)\n",
    "- **Database Schema**: See `producthuntdb/models.py` for complete schema\n",
    "- **Configuration Options**: See `producthuntdb/config.py` for all settings\n",
    "\n",
    "## üõ†Ô∏è Troubleshooting Guide\n",
    "\n",
    "### Common Errors and Solutions\n",
    "\n",
    "#### 1. Authentication Errors\n",
    "\n",
    "**Error**: `Authentication failed` or `Invalid token`\n",
    "\n",
    "**Solutions**:\n",
    "```python\n",
    "# Verify token is set correctly\n",
    "import os\n",
    "token = os.getenv(\"PRODUCTHUNT_TOKEN\")\n",
    "print(f\"Token configured: {bool(token)}\")\n",
    "print(f\"Token length: {len(token) if token else 0} chars\")\n",
    "\n",
    "# Expected: Token configured: True, Length: ~100 chars\n",
    "```\n",
    "\n",
    "- Get a new token at: https://api.producthunt.com/v2/oauth/applications\n",
    "- In Kaggle: Settings ‚Üí Add-ons ‚Üí Secrets ‚Üí Add `PRODUCTHUNT_TOKEN`\n",
    "- Verify no extra spaces or newlines in token\n",
    "- Token should start with `\"Bearer \"` prefix or be the raw OAuth token\n",
    "\n",
    "#### 2. Database Locked\n",
    "\n",
    "**Error**: `database is locked` or `OperationalError: database is locked`\n",
    "\n",
    "**Solutions**:\n",
    "```bash\n",
    "# Option 1: Close other connections and retry\n",
    "# In Python cell:\n",
    "import sqlite3, os\n",
    "conn = sqlite3.connect(os.getenv('DB_PATH'))\n",
    "conn.close()  # Ensure previous connections are closed\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Option 2: Reset database (warning: deletes all data)\n",
    "!rm -f /kaggle/working/producthunt.db*\n",
    "!producthuntdb init\n",
    "```\n",
    "\n",
    "**Prevention**:\n",
    "- Always use `try/finally` blocks to close connections\n",
    "- Avoid running multiple sync operations simultaneously\n",
    "- Enable WAL mode for better concurrency (automatic in v0.1.0+)\n",
    "\n",
    "#### 3. Rate Limiting\n",
    "\n",
    "**Error**: `rate limit exceeded` or `429 Too Many Requests`\n",
    "\n",
    "**Solutions**:\n",
    "- Built-in retry logic handles this automatically (exponential backoff)\n",
    "- Reduce `--max-pages` for testing\n",
    "- Run sync during off-peak hours (late night/early morning UTC)\n",
    "- Wait 15-30 minutes between full refresh attempts\n",
    "\n",
    "**Check rate limit status**:\n",
    "```python\n",
    "import subprocess\n",
    "result = subprocess.run(['producthuntdb', 'verify'], capture_output=True, text=True)\n",
    "print(result.stdout)  # Shows API status and rate limits\n",
    "```\n",
    "\n",
    "#### 4. Memory Errors\n",
    "\n",
    "**Error**: `MemoryError` or `Out of memory`\n",
    "\n",
    "**Solutions**:\n",
    "```python\n",
    "# Use chunked reading for large queries\n",
    "import pandas as pd\n",
    "import sqlite3, os\n",
    "\n",
    "conn = sqlite3.connect(os.getenv('DB_PATH'))\n",
    "chunks = []\n",
    "for chunk in pd.read_sql_query(\"SELECT * FROM post_row\", conn, chunksize=1000):\n",
    "    # Process chunk\n",
    "    chunks.append(chunk.head(10))  # Example: keep only top 10 per chunk\n",
    "result = pd.concat(chunks)\n",
    "conn.close()\n",
    "```\n",
    "\n",
    "**Prevention**:\n",
    "- Use `LIMIT` clauses in queries\n",
    "- Enable Parquet export (more memory efficient)\n",
    "- Clear old notebook outputs: Cell ‚Üí All Output ‚Üí Clear\n",
    "\n",
    "#### 5. Timeout Errors\n",
    "\n",
    "**Error**: `TimeoutExpired` or sync takes >12 hours\n",
    "\n",
    "**Solutions**:\n",
    "- Use incremental sync instead of `--full-refresh`\n",
    "- Split into smaller batches with `--max-pages 100`\n",
    "- Run posts-only sync first: `--posts-only`\n",
    "- Data is saved progressively - re-run to continue\n",
    "\n",
    "**Monitor progress**:\n",
    "```bash\n",
    "# Check what was synced so far\n",
    "!producthuntdb status\n",
    "```\n",
    "\n",
    "#### 6. Import Errors\n",
    "\n",
    "**Error**: `ModuleNotFoundError: No module named 'producthuntdb'`\n",
    "\n",
    "**Solutions**:\n",
    "```bash\n",
    "# Reinstall package\n",
    "!pip uninstall -y producthuntdb\n",
    "!pip install -q git+https://github.com/wyattowalsh/producthuntdb.git\n",
    "\n",
    "# Verify installation\n",
    "!python -c \"import producthuntdb; print('OK')\"\n",
    "```\n",
    "\n",
    "#### 7. Database Schema Errors\n",
    "\n",
    "**Error**: `no such table` or `no such column`\n",
    "\n",
    "**Solutions**:\n",
    "```bash\n",
    "# Check current schema version\n",
    "!producthuntdb migration-history\n",
    "\n",
    "# Apply pending migrations\n",
    "!producthuntdb upgrade head\n",
    "\n",
    "# If corrupted, reinitialize (warning: deletes data)\n",
    "!rm -f /kaggle/working/producthunt.db*\n",
    "!producthuntdb init\n",
    "```\n",
    "\n",
    "#### 8. Export Failures\n",
    "\n",
    "**Error**: Export command fails or produces empty files\n",
    "\n",
    "**Solutions**:\n",
    "```python\n",
    "# Check database has data\n",
    "import sqlite3, os, pandas as pd\n",
    "conn = sqlite3.connect(os.getenv('DB_PATH'))\n",
    "count = pd.read_sql_query(\"SELECT COUNT(*) as cnt FROM post_row\", conn)\n",
    "print(f\"Posts in database: {count['cnt'].iloc[0]}\")\n",
    "conn.close()\n",
    "\n",
    "# If count = 0, run sync first\n",
    "```\n",
    "\n",
    "**Manual export**:\n",
    "```python\n",
    "# Export specific table manually\n",
    "import pandas as pd, sqlite3, os\n",
    "from pathlib import Path\n",
    "\n",
    "conn = sqlite3.connect(os.getenv('DB_PATH'))\n",
    "df = pd.read_sql_query(\"SELECT * FROM post_row\", conn)\n",
    "export_dir = Path(os.getenv('EXPORT_DIR'))\n",
    "export_dir.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(export_dir / \"post_row.csv\", index=False)\n",
    "print(f\"Exported {len(df)} rows to post_row.csv\")\n",
    "conn.close()\n",
    "```\n",
    "\n",
    "#### 9. Kaggle Publishing Errors\n",
    "\n",
    "**Error**: Publishing to Kaggle fails\n",
    "\n",
    "**Solutions**:\n",
    "- Verify all three secrets are set:\n",
    "  - `KAGGLE_USERNAME`\n",
    "  - `KAGGLE_KEY` (from kaggle.com/settings ‚Üí API ‚Üí Create New Token)\n",
    "  - `KAGGLE_DATASET_SLUG` (format: `username/dataset-name`)\n",
    "- Dataset must exist on Kaggle first (create manually or let CLI create it)\n",
    "- Check you have write permissions to the dataset\n",
    "\n",
    "**Test credentials**:\n",
    "```python\n",
    "import os\n",
    "creds = {\n",
    "    'KAGGLE_USERNAME': bool(os.getenv('KAGGLE_USERNAME')),\n",
    "    'KAGGLE_KEY': bool(os.getenv('KAGGLE_KEY')),\n",
    "    'KAGGLE_DATASET_SLUG': bool(os.getenv('KAGGLE_DATASET_SLUG'))\n",
    "}\n",
    "print(\"Kaggle credentials configured:\", all(creds.values()))\n",
    "for key, val in creds.items():\n",
    "    print(f\"  {key}: {'‚úÖ' if val else '‚ùå'}\")\n",
    "```\n",
    "\n",
    "### Performance Optimization Tips\n",
    "\n",
    "1. **First Run**: Use `--full-refresh` to get all historical data (2-4 hours)\n",
    "2. **Daily Updates**: Use default `sync` command (3-5 minutes)\n",
    "3. **Testing**: Use `--max-pages 10` to sync quickly\n",
    "4. **Selective Sync**: Use `--posts-only` to skip topics/collections\n",
    "5. **Monitoring**: Check `sync_history.txt` for performance trends\n",
    "\n",
    "### Getting Help\n",
    "\n",
    "**Debug mode** (verbose logging):\n",
    "```bash\n",
    "# Enable debug logging (if supported)\n",
    "!producthuntdb --verbose sync\n",
    "```\n",
    "\n",
    "**Check system status**:\n",
    "```python\n",
    "import sys, platform, os\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Working dir: {Path.cwd()}\")\n",
    "print(f\"DB Path: {os.getenv('DB_PATH')}\")\n",
    "print(f\"Disk free: {shutil.disk_usage('.').free / 1024**3:.1f} GB\")\n",
    "```\n",
    "\n",
    "**Report issues**:\n",
    "- **GitHub Issues**: [github.com/wyattowalsh/producthuntdb/issues](https://github.com/wyattowalsh/producthuntdb/issues)\n",
    "- Include error messages, Python version, and notebook output\n",
    "- Check existing issues first for known problems\n",
    "\n",
    "## ü§ù Contributing\n",
    "\n",
    "Found a bug or have a feature request?\n",
    "\n",
    "- **Issues**: [GitHub Issues](https://github.com/wyattowalsh/producthuntdb/issues)\n",
    "- **Pull Requests**: Contributions welcome!\n",
    "\n",
    "## üìÑ License\n",
    "\n",
    "MIT License - see [LICENSE](https://github.com/wyattowalsh/producthuntdb/blob/main/LICENSE) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14536301",
   "metadata": {},
   "source": [
    "# ‚úÖ Production Checklist\n",
    "\n",
    "Before scheduling this notebook on Kaggle, verify:\n",
    "\n",
    "- [ ] **Secrets Configured** - `PRODUCTHUNT_TOKEN` is set in Kaggle Secrets\n",
    "- [ ] **First Run Complete** - Ran once with `--full-refresh` to populate database\n",
    "- [ ] **Export Works** - CSV files generated successfully\n",
    "- [ ] **Database Valid** - `producthuntdb status` shows expected data counts\n",
    "- [ ] **(Optional) Publishing Works** - Kaggle credentials configured and tested\n",
    "\n",
    "Once checked, you're ready to schedule! üéâ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "producthuntdb (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
